{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 11 - Regular Expression\n",
    "Regular expressions are a very specialized language that allow us to succinctly search strings and extract data from strings. Regular expressions are a language unto themselves. It is not essential to know how to use regular expressions, but they can be quite useful and powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- really cleaver 'wild card' expression \n",
    "- very porwerful and very cryptic\n",
    "- fun once  you  understand them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `^` Matches the beginning of a line\n",
    "-  `$` Matched the end of the line\n",
    "- `.` Matches any character\n",
    "- `\\s` Matches whitespaces\n",
    "-  `\\S` Matches any non-whitespace character\n",
    "- `*` Repeats a character zero or more  times\n",
    "- `*?` Repeats a character zero  or more times (non greedy)\n",
    "- `+` Repeats a character one or more times\n",
    "- `+?` Repeats a character one or  more times (non greedy)\n",
    "- `[aeiou]` Matches single character in the listed set\n",
    "- `[^XYZ]` Matches a single character not in the listed  set\n",
    "- `[a-z0-9]` The set of characters can include a range\n",
    "- `(` Indicates where string extraction is to start\n",
    "- `)` Indicates where string extraction is to end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import re`  - imports the regular expression in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2 Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.search()` - returns a True/False depending on whether the string matches the regular expression\n",
    "\n",
    "`re.findall()` - extracts the matching string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '19', '42']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'My 2 favarite numbers are 19 and 42'\n",
    "y = re.findall('[0-9]+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# continuation\n",
    "# find if there is any uppercase vowel\n",
    "y = re.findall('[AEIOU]+', x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the:']\n"
     ]
    }
   ],
   "source": [
    "# Greedy Matching\n",
    "xx = 'From: Using the: character'\n",
    "y = re.findall('^F.+:', xx)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:']\n"
     ]
    }
   ],
   "source": [
    "## Non Greedy Match\n",
    "y = re.findall('^F.+?:', xx)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning String Extraction\n",
    "You can refine the match for `re.findlall()` and seperately determine which portion of the mactch is to be extracted using parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random@mail.com.us']\n"
     ]
    }
   ],
   "source": [
    "x = 'From random@mail.com.us Sat June 5 09:94:44 20114'\n",
    "y = re.findall('\\S+@\\S+', x)\n",
    "\n",
    "# This is greedy extraction\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random@mail.com.us']\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning the above example\n",
    "y = re.findall('^From (\\S+@\\S+)', x)\n",
    "\n",
    "# Matchihg and demanding exact email address\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1 - `@([^ ]*)`\n",
    "- `[^ ]` = Match non-blank character = everything but space\n",
    "- `*` = Match many of them\n",
    "-  `@( )` = Match everything after `@` but don't extract `@`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2 - `^From .*@([^ ]*)`\n",
    "- `[^ ]` = Match non-blank character = everything but space\n",
    "- `*` = Match many of them\n",
    "-  ` .*@( )` = Match everything after 'From' upto `@`, and extract after that\n",
    "- `^` = Start at the beginning of the line\n",
    "- `^From` = Look for the string 'From'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3 - `re.findall('^X-DSPAM-Confidence: ([0-9.]+)' ,line)`\n",
    "- `^X-DSPAM-Confidence:` = Find everything that starts with this, followed by a space. Then extract whats in the parentheses `()`\n",
    "- `([0-9.]+)` = Look for 0-9 numbers with period =  floating number, `+` = one or more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file:\n",
      "432049\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "name = input(\"Enter file:\")\n",
    "if len(name) < 1 : name = \"regex_sum_806196.txt\"\n",
    "handle = open(name)\n",
    "\n",
    "sum_list = list()\n",
    "another_list = list()\n",
    "\n",
    "for line in handle:\n",
    "    line = line.rstrip()\n",
    "    x = re.findall('[0-9*]+' , line)\n",
    "    #print(x)\n",
    "    for num in x:\n",
    "        if len(num) > 0:\n",
    "            sum_list.append(int(num))\n",
    "\n",
    "print(sum(sum_list))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular expression example codes\n",
    "\n",
    "HTML links = `re.findall('href=\"(http[s]?://.*)\"',line)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 12 - Network Technology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step One - Make Connection - [Transport Protocol]\n",
    "Python has built-in support for TCP Sockets. jsut pass host name and port number.\n",
    "\n",
    "`import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Two  - Application Protocol\n",
    "HTTP (Hypertext Transfer Protocol) - has request-responce  cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\r\n",
      "Date: Tue, 21 Jul 2020 21:03:49 GMT\r\n",
      "Server: Apache/2.4.18 (Ubuntu)\r\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\r\n",
      "ETag: \"1d3-54f6609240717\"\r\n",
      "Accept-Ranges: bytes\r\n",
      "Content-Length: 467\r\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\r\n",
      "Pragma: no-cache\r\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\r\n",
      "Connection: close\r\n",
      "Content-Type: text/plain\r\n",
      "\r\n",
      "Why should you learn to write programs?\n",
      "\n",
      "Writing programs (or programming) is a very creative \n",
      "and rewarding activity.  You can write programs\n",
      " for \n",
      "many reasons, ranging from making your living to solving\n",
      "a difficult data analysis problem to having fun to helping\n",
      "someone else solve a problem.  This book assumes that \n",
      "everyone needs to know how to program, and that once \n",
      "you know how to program you will figure out what you want \n",
      "to do with your newfound skills.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# Step One  - Make a connection\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# Step Two - Send a GET request\n",
    "# send a HTTP command\n",
    "cmd = 'GET http://data.pr4e.org/intro-short.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "mysock.send(cmd)\n",
    "\n",
    "# Step Three - Get the data back\n",
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    print(data.decode())\n",
    "# close the connection\n",
    "mysock.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encode()` and `.decode()` - are two strings commands that converts and decodes unocode to utf8 and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.3 Unicode Characters and Strings\n",
    "- each character is represented by a number between 0 and 256 stored in 8 bits of memory.\n",
    "- We refer to \"8 bits\" of memory as a *byte* of memory\n",
    "- The `ord()` function tells us the numberic value of a simple ASCII character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "print(ord('H'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the wide range of characters coputers must handle we represent characters with more than one byte.\n",
    "- UTF-8 is recommended practice for endcoding data to be exhanged between systems.\n",
    "- In  python3 all the strings internally   are UNICODE\n",
    "- Working with strings variables in Python programs are reading data from files usually \"Just works\". \n",
    "- When python talks to a network resource using sockets or talks to a database we have to encode and decode (usualy to UTF-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python stringd to Bytes\n",
    "\n",
    "When we talk to an external resources like a netwerk socket we send bytes, so we need to encode python3 strings to a given character encoding. Similarly, when we read data from an extrenal resources, we must decode it based on character se so it is properly represented in python3 as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    data = mysock.recv(512)\n",
    "    if (len(data) < 1):\n",
    "        break\n",
    "    mystring = data.decode()\n",
    "    print(mystring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.4 Retrieving Web Pages\n",
    "Since HTTP is so common, python3 has a library that does all the socket work for it and makes web page/s look like a  file. `urllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.5 Parsing Web Pages\n",
    "#### Web Scraping\n",
    "When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - \n",
      "http://www.google.com/imghp?hl=en&tab=wi\n",
      "http://maps.google.com/maps?hl=en&tab=wl\n",
      "https://play.google.com/?hl=en&tab=w8\n",
      "http://www.youtube.com/?gl=US&tab=w1\n",
      "http://news.google.com/nwshp?hl=en&tab=wn\n",
      "https://mail.google.com/mail/?tab=wm\n",
      "https://drive.google.com/?tab=wo\n",
      "https://www.google.com/intl/en/about/products?tab=wh\n",
      "http://www.google.com/history/optout?hl=en\n",
      "/preferences?hl=en\n",
      "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
      "/advanced_search?hl=en&authuser=0\n",
      "/intl/en/ads/\n",
      "/services/\n",
      "/intl/en/about.html\n",
      "/intl/en/policies/privacy/\n",
      "/intl/en/policies/terms/\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter - ')\n",
    "if len(url) < 1 : url = \"http://www.google.com/\"\n",
    "    \n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - \n",
      "http://www.google.com/imghp?hl=en&tab=wi\n",
      "http://maps.google.com/maps?hl=en&tab=wl\n",
      "https://play.google.com/?hl=en&tab=w8\n",
      "http://www.youtube.com/?gl=US&tab=w1\n",
      "http://news.google.com/nwshp?hl=en&tab=wn\n",
      "https://mail.google.com/mail/?tab=wm\n",
      "https://drive.google.com/?tab=wo\n",
      "https://www.google.com/intl/en/about/products?tab=wh\n",
      "http://www.google.com/history/optout?hl=en\n",
      "/preferences?hl=en\n",
      "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
      "/advanced_search?hl=en&authuser=0\n",
      "/intl/en/ads/\n",
      "/services/\n",
      "/intl/en/about.html\n",
      "/intl/en/policies/privacy/\n",
      "/intl/en/policies/terms/\n",
      "********\n",
      "http://www.youtube.com/?gl=US&tab=w1\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "url = input('Enter - ')\n",
    "if len(url) < 1 : url = \"http://www.google.com/\"\n",
    "    \n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "    \n",
    "html = urllib.request.urlopen(url, context = ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # print(tag)\n",
    "    print(tag.get('href', None))\n",
    " \n",
    "print('********')\n",
    "# Print specific Tag\n",
    "print(tags[3].get('href', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use BeautifulSoup to pull out various parts of each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - \n",
      "TAG: <a class=\"gb1\" href=\"http://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>\n",
      "URL: http://www.google.com/imghp?hl=en&tab=wi\n",
      "Contents: Images\n",
      "Attrs: {'class': ['gb1'], 'href': 'http://www.google.com/imghp?hl=en&tab=wi'}\n",
      "TAG: <a class=\"gb1\" href=\"http://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>\n",
      "URL: http://maps.google.com/maps?hl=en&tab=wl\n",
      "Contents: Maps\n",
      "Attrs: {'class': ['gb1'], 'href': 'http://maps.google.com/maps?hl=en&tab=wl'}\n",
      "TAG: <a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>\n",
      "URL: https://play.google.com/?hl=en&tab=w8\n",
      "Contents: Play\n",
      "Attrs: {'class': ['gb1'], 'href': 'https://play.google.com/?hl=en&tab=w8'}\n",
      "TAG: <a class=\"gb1\" href=\"http://www.youtube.com/?gl=US&amp;tab=w1\">YouTube</a>\n",
      "URL: http://www.youtube.com/?gl=US&tab=w1\n",
      "Contents: YouTube\n",
      "Attrs: {'class': ['gb1'], 'href': 'http://www.youtube.com/?gl=US&tab=w1'}\n",
      "TAG: <a class=\"gb1\" href=\"http://news.google.com/nwshp?hl=en&amp;tab=wn\">News</a>\n",
      "URL: http://news.google.com/nwshp?hl=en&tab=wn\n",
      "Contents: News\n",
      "Attrs: {'class': ['gb1'], 'href': 'http://news.google.com/nwshp?hl=en&tab=wn'}\n",
      "TAG: <a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a>\n",
      "URL: https://mail.google.com/mail/?tab=wm\n",
      "Contents: Gmail\n",
      "Attrs: {'class': ['gb1'], 'href': 'https://mail.google.com/mail/?tab=wm'}\n",
      "TAG: <a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a>\n",
      "URL: https://drive.google.com/?tab=wo\n",
      "Contents: Drive\n",
      "Attrs: {'class': ['gb1'], 'href': 'https://drive.google.com/?tab=wo'}\n",
      "TAG: <a class=\"gb1\" href=\"https://www.google.com/intl/en/about/products?tab=wh\" style=\"text-decoration:none\"><u>More</u> »</a>\n",
      "URL: https://www.google.com/intl/en/about/products?tab=wh\n",
      "Contents: <u>More</u>\n",
      "Attrs: {'class': ['gb1'], 'style': 'text-decoration:none', 'href': 'https://www.google.com/intl/en/about/products?tab=wh'}\n",
      "TAG: <a class=\"gb4\" href=\"http://www.google.com/history/optout?hl=en\">Web History</a>\n",
      "URL: http://www.google.com/history/optout?hl=en\n",
      "Contents: Web History\n",
      "Attrs: {'href': 'http://www.google.com/history/optout?hl=en', 'class': ['gb4']}\n",
      "TAG: <a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a>\n",
      "URL: /preferences?hl=en\n",
      "Contents: Settings\n",
      "Attrs: {'href': '/preferences?hl=en', 'class': ['gb4']}\n",
      "TAG: <a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=http://www.google.com/\" id=\"gb_70\" target=\"_top\">Sign in</a>\n",
      "URL: https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
      "Contents: Sign in\n",
      "Attrs: {'target': '_top', 'id': 'gb_70', 'href': 'https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/', 'class': ['gb4']}\n",
      "TAG: <a href=\"/advanced_search?hl=en&amp;authuser=0\">Advanced search</a>\n",
      "URL: /advanced_search?hl=en&authuser=0\n",
      "Contents: Advanced search\n",
      "Attrs: {'href': '/advanced_search?hl=en&authuser=0'}\n",
      "TAG: <a href=\"/intl/en/ads/\">Advertising Programs</a>\n",
      "URL: /intl/en/ads/\n",
      "Contents: Advertising Programs\n",
      "Attrs: {'href': '/intl/en/ads/'}\n",
      "TAG: <a href=\"/services/\">Business Solutions</a>\n",
      "URL: /services/\n",
      "Contents: Business Solutions\n",
      "Attrs: {'href': '/services/'}\n",
      "TAG: <a href=\"/intl/en/about.html\">About Google</a>\n",
      "URL: /intl/en/about.html\n",
      "Contents: About Google\n",
      "Attrs: {'href': '/intl/en/about.html'}\n",
      "TAG: <a href=\"/intl/en/policies/privacy/\">Privacy</a>\n",
      "URL: /intl/en/policies/privacy/\n",
      "Contents: Privacy\n",
      "Attrs: {'href': '/intl/en/policies/privacy/'}\n",
      "TAG: <a href=\"/intl/en/policies/terms/\">Terms</a>\n",
      "URL: /intl/en/policies/terms/\n",
      "Contents: Terms\n",
      "Attrs: {'href': '/intl/en/policies/terms/'}\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "if len(url) < 1 : url = \"http://www.google.com\"\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignement: Scraping Numbers from HTML using BeautifulSoup \n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment. You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - \n",
      "2688\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "\n",
    "url = input('Enter - ')\n",
    "if len(url) < 1 : url = \"http://py4e-data.dr-chuck.net/comments_806198.html\"\n",
    "html = urlopen(url).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "collection = list()\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span')\n",
    "for tag in tags:\n",
    "    # print(tag.contents[0])\n",
    "    collection.append(int(tag.contents[0]))\n",
    "\n",
    "print(sum(collection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment: Following Links in HTML Using BeautifulSoup\n",
    "In this assignment you will write a Python program that expands on https://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= values from the anchor tags, scan for a tag that is in a particular position from the top and follow that link, repeat the process a number of times, and report the last name you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - \n",
      "Enter position: 3\n",
      "Enter count: 3\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Ridley.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Edwin.html\n",
      "Retrieving:  http://py4e-data.dr-chuck.net/known_by_Morran.html\n",
      "*****************\n",
      "Last name in sequence:  Morran\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')\n",
    "if len(url) < 1 : \n",
    "    # Creating default value for assignement\n",
    "    url = \"http://py4e-data.dr-chuck.net/known_by_Pietro.html\"\n",
    "    \n",
    "position = int(input(\"Enter position: \"))-1\n",
    "count = int(input(\"Enter count: \"))\n",
    "\n",
    "\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Creating a List of names\n",
    "Sequence = list()\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "\n",
    "for name in range(count):\n",
    "    # Step 1 - get the specific positioned link\n",
    "    link = tags[position].get('href', None)\n",
    "    # Step 2 - print the link\n",
    "    print(\"Retrieving: \",link)\n",
    "    # Step 3 - append this particular name on the sequence\n",
    "    Sequence.append(tags[position].contents[0])\n",
    "    # Step 4 - open page --> to the positioned link and redo\n",
    "    html = urllib.request.urlopen(link, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "\n",
    "# print the Last name of the sequence\n",
    "print('*****************')\n",
    "print('Last name in sequence: ', Sequence[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 13  - Data on Web\n",
    "In this section, we learn how to retrieve and parse XML (eXtensible Markup Language) data.\n",
    "\n",
    "With the HTTP Request/Response well understood and well supported, there was a natural move toward exchanging data between programs using these protocols. There are two commonly used data formats, that can be used between applications and accross networks --> XML and JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialization\n",
    "Act of taking a data from one program or programming language to a more widely used 'wired-data' format.\n",
    "#### Deserialization\n",
    "Act of bringing back the 'wired-data' format into program specific or programming language speficic format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML data format - eXtensible Markup Language\n",
    "- primary purpose - share structured data\n",
    "- Component -\n",
    "- - Start Tag, End Tag\n",
    "- - Attrribtutes (always in start tag)\n",
    "- - Content\n",
    "- - Self Closing Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.3 XML Schema\n",
    "- Describing  a  'contract' as  to  what  is acceptable XML\n",
    "- Way to sort of establish outside of program/programming language \n",
    "- This helps to  validate the data that  needs to be unerstood before sending/receiving data.\n",
    "- XML Schema from W3C - (XSD) - popular one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.4 Parsing XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:   Prashant \n",
      "Attr:   yes\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "data =  '''\n",
    "<person> \n",
    "<name> Prashant </name>\n",
    "<phone> +1 347 204 00044</phone>\n",
    "<email hide =  \"yes\"/>\n",
    "</person>\n",
    "'''\n",
    "\n",
    "tree = ET.fromstring(data)\n",
    "print('Name: ', tree.find('name').text)\n",
    "print('Attr:  ', tree.find('email').get('hide'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Count:  2\n",
      "Name:    Prashant \n",
      "Email  Attr:  yes\n",
      "    - \n",
      "Name:    Another Prashant \n",
      "Email  Attr:  yes\n",
      "    - \n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "input =  ''' <stuff>\n",
    "<users> \n",
    "\n",
    "<user x = \"2\">\n",
    "<name> Prashant </name>\n",
    "<phone> +1 347 204 00044</phone>\n",
    "<email hide =  \"yes\"/>\n",
    "</user>\n",
    "\n",
    "<user x = \"7\">\n",
    "<name> Another Prashant </name>\n",
    "<phone> +1 347 204 3344</phone>\n",
    "<email hide =  \"yes\"/>\n",
    "</user>\n",
    "\n",
    "</users>\n",
    "</stuff>  '''\n",
    "\n",
    "stuff = ET.fromstring(input)\n",
    "something = stuff.findall('users/user')\n",
    "print('User Count: ', len(something) )\n",
    "\n",
    "for item in something:\n",
    "    print ('Name:  ', item.find('name').text)\n",
    "    print ('Email  Attr: ',  item.find('email').get('hide'))\n",
    "    print ('    - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment-  Extracting Data from XML\n",
    "In this assignment you will write a Python program somewhat similar to https://py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file and enter the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter - \n",
      "Retrieved   4206  charatcters\n",
      "Count:  50\n",
      "Sum:  2468\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import xml.etree.ElementTree as ET\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Enter the URL\n",
    "url2 = input('Enter - ')\n",
    "if len(url2) < 1 : \n",
    "    # Creating default value for assignement\n",
    "    url2 = \"http://py4e-data.dr-chuck.net/comments_806200.xml\"\n",
    "\n",
    "html2 = urllib.request.urlopen(url2, context=ctx).read()\n",
    "print('Retrieved  ',len(html2), ' charatcters')\n",
    "\n",
    "tree = ET.fromstring(html2)\n",
    "something = tree.findall('comments/comment')\n",
    "print('Count: ', len(something))\n",
    "\n",
    "generic_list = list()\n",
    "\n",
    "for item  in something:\n",
    "    generic_list.append(int(item.find('count').text))\n",
    "    \n",
    "print('Sum: ', sum(generic_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... same  thing  -  -  less code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4206 characters\n",
      "Sum:   2468\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "url = 'http://py4e-data.dr-chuck.net/comments_806200.xml'\n",
    "\n",
    "\n",
    "html2 = urllib.request.urlopen(url2, context=ctx).read()\n",
    "print('Retrieved',len(html2),'characters')\n",
    "\n",
    "tree = ET.fromstring(html2)\n",
    "\n",
    "\n",
    "results = tree.findall('.//count')\n",
    "nums = [int(node.text) for node in results]\n",
    "counts = sum(nums)\n",
    "print('Sum:  ', counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.5 JavaScript Object Notation  (JSON)\n",
    "- JSON represents data as  nested 'lists' and  'dictionaries'\n",
    "- in python3, the json.load  gives  you either  list or  dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:  Chuck\n",
      "Hide:  yes\n"
     ]
    }
   ],
   "source": [
    "import  json\n",
    "data = '''\n",
    "{\n",
    "\"name\"  : \"Chuck\",\n",
    "\"phone\" : {\n",
    "\"type\" : \"intl\",\n",
    "\"number\": \"+1 234 567 8900\"\n",
    "},\n",
    "\"email\" : {\n",
    "\"hide\" : \"yes\"\n",
    "}\n",
    "}\n",
    "'''\n",
    "\n",
    "info  =  json.loads(data)\n",
    "\n",
    "#  the  'info' here  is  a python dictionary\n",
    "print('Name: ', info[\"name\"])\n",
    "print('Hide: ', info[\"email\"][\"hide\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.6 Service Oriented Approach (SOA)\n",
    "By introducing service layers between systems, Service  Oriented  Approach helps multiple systems  connect  each other and  provide  data. SOA defines  and documents \"contracts\" between different applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.7 Application Programming Interface (API)\n",
    "The general name for application to  application contracts is Application Program  Interface (API). When we use an  API, generally one  program makes a set of  services available  for use  by other applicaitons and publishes  the APIs that must be  followed  to access the  services provided by the  program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.8  Secure API Requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment - Extracting Data from JSON\n",
    "The program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: http://py4e-data.dr-chuck.net/comments_806201.json\n",
      "Retrieving  http://py4e-data.dr-chuck.net/comments_806201.json\n",
      "Retrieved   2716  charatcters\n",
      "Count: 50\n",
      "Sum: 2285\n",
      "Enter location: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# URL = http://py4e-data.dr-chuck.net/comments_806201.json\n",
    "\n",
    "while True:\n",
    "    url3 = input('Enter location: ')\n",
    "    if len(url3) < 1 : break\n",
    "\n",
    "    # Print URL\n",
    "    print ('Retrieving ', url3)\n",
    "    \n",
    "    # Print  Characters\n",
    "    html3 = urllib.request.urlopen(url3, context=ctx).read()\n",
    "    print('Retrieved  ',len(html3), ' charatcters')\n",
    "    \n",
    "    ## Print Count\n",
    "    info = json.loads(html3)\n",
    "    comments = info[\"comments\"]\n",
    "    print ('Count:', len(comments))\n",
    "    \n",
    "    # Print Sum\n",
    "    sum = 0\n",
    "    for item in comments:\n",
    "        sum = sum + item['count']\n",
    "    print ('Sum:', sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment - Using the GeoJSON API\n",
    "In this program you will use a GeoLocation lookup API modelled after the Google API to look up some universities and parse the returned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter location: University of Oxford\n",
      "Retrieving http://py4e-data.dr-chuck.net/json?address=University+of+Oxford&key=42\n",
      "Retrieved 1773 characters\n",
      "Place id  ChIJW0iM76nGdkgR7a8BoIMY_9I\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import json\n",
    "\n",
    "api_key = False\n",
    "\n",
    "if api_key is False:\n",
    "    api_key = 42\n",
    "    serviceurl = 'http://py4e-data.dr-chuck.net/json?'\n",
    "else :\n",
    "    serviceurl = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Address - University of Oxford\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1 : break\n",
    "    \n",
    "    parms = dict()\n",
    "    parms['address'] = address\n",
    "    if api_key is not False: parms['key'] = api_key\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "    print ('Retrieving', url)\n",
    "    \n",
    "    \n",
    "    # Print the Retrieve lenght of data. \n",
    "    # This data is line by line loading of received data\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "    \n",
    "\n",
    "    # Now, we load data into list/dictionary format via json.load()\n",
    "    # This step changes the data to python redeable list/dictionary\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "    \n",
    "    if not js or 'status' not in js or js['status'] != 'OK':\n",
    "        print('==== Failure To Retrieve ====')\n",
    "        print(data)\n",
    "        continue\n",
    "    \n",
    "    # This will pretty print json for more clarity\n",
    "    # print(json.dumps(js, indent=4))\n",
    "    \n",
    "    place_id = js['results'][0]['place_id']\n",
    "    print('Place id ', place_id)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
